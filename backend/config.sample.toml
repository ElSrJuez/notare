# NotƒÅre configuration sample
[llm]
# Choose provider: "openai" or "llama"
provider = "openai"

[llm.openai]
api_key = "YOUR_OPENAI_API_KEY"
model   = "gpt-4o-chat-bison"

[llm.llama]
# HTTP endpoint for llama.cpp server or similar
endpoint = "http://localhost:8080/completions"
model    = "llama-2-7b-chat"
